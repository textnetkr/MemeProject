{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyMJfsh7Ld7fLa03PauZeZWT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"premium","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Install"],"metadata":{"id":"FN-Jm9IgyynN"}},{"cell_type":"code","source":["!pip3 install transformers\n","!pip3 install datasets\n","!pip3 install evaluate\n","!pip3 install torch"],"metadata":{"id":"cEWPHH3o6CZk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip3 install pshmodule\n","!pip3 install wandb\n","!pip3 install pandas==1.5.0\n","!pip3 install pickle5\n","!pip3 install hydra-core"],"metadata":{"id":"yzC54sGWPm3t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"qqomZDQhz488","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677557610945,"user_tz":-540,"elapsed":104520,"user":{"displayName":"ë°•ì„±í™˜","userId":"16608640204304641951"}},"outputId":"c1b1ca53-0575-4adb-b660-361b6f3f012c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip3 list | grep datasets"],"metadata":{"id":"zschQKscJlM5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677557611564,"user_tz":-540,"elapsed":625,"user":{"displayName":"ë°•ì„±í™˜","userId":"16608640204304641951"}},"outputId":"d763862c-c315-4301-a590-0c14d735d5b3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["datasets                      2.10.0\n","tensorflow-datasets           4.8.2\n","vega-datasets                 0.9.0\n"]}]},{"cell_type":"markdown","source":["# GPU Check"],"metadata":{"id":"Fiap6RttXJza"}},{"cell_type":"code","source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LXqlHhTzXz49","executionInfo":{"status":"ok","timestamp":1677557613444,"user_tz":-540,"elapsed":1881,"user":{"displayName":"ë°•ì„±í™˜","userId":"16608640204304641951"}},"outputId":"184f5664-99d7-4417-9981-56d07f47a3a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tue Feb 28 04:13:31 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   32C    P0    51W / 400W |      0MiB / 40960MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lbRds2i6X5Hl","executionInfo":{"status":"ok","timestamp":1677557613444,"user_tz":-540,"elapsed":3,"user":{"displayName":"ë°•ì„±í™˜","userId":"16608640204304641951"}},"outputId":"c27eedc7-2d38-4d95-9824-1c6114442e8f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Your runtime has 89.6 gigabytes of available RAM\n","\n","You are using a high-RAM runtime!\n"]}]},{"cell_type":"markdown","source":["# Train"],"metadata":{"id":"1D-Ls5bAXRVN"}},{"cell_type":"code","source":["import os\n","os.chdir('drive/MyDrive/MemeProject/src/pytorch/augmentation/transform')"],"metadata":{"id":"ACD6i5ZdXJGa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !python3 train.py"],"metadata":{"id":"Jwb_jpszVEHL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python3 train_hf.py"],"metadata":{"id":"52LwcvGJl1G6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677558891344,"user_tz":-540,"elapsed":3765,"user":{"displayName":"ë°•ì„±í™˜","userId":"16608640204304641951"}},"outputId":"b54e3dbb-eac4-4ed4-ad0f-3f7807b98074"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["2023-02-28 04:13:41.734759: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-02-28 04:13:41.865239: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2023-02-28 04:13:42.630839: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-02-28 04:13:42.630936: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-02-28 04:13:42.630966: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","Downloading (â€¦)lve/main/config.json: 100% 1.36k/1.36k [00:00<00:00, 207kB/s]\n","You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n","Downloading (â€¦)/main/tokenizer.json: 100% 682k/682k [00:00<00:00, 1.59MB/s]\n","Downloading (â€¦)in/added_tokens.json: 100% 4.00/4.00 [00:00<00:00, 1.47kB/s]\n","Downloading (â€¦)cial_tokens_map.json: 100% 112/112 [00:00<00:00, 44.4kB/s]\n","You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n","Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-135468cdf5c23cdc/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n","Downloading data files: 100% 1/1 [00:00<00:00, 2863.01it/s]\n","Extracting data files: 100% 1/1 [00:01<00:00,  1.60s/it]\n","Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-135468cdf5c23cdc/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n","100% 2/2 [00:00<00:00, 294.23it/s]\n","You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n","Downloading (â€¦)\"pytorch_model.bin\";: 100% 496M/496M [00:05<00:00, 97.3MB/s]\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.10\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/MemeProject/src/pytorch/augmentation/outputs/2023-02-28/04-13-50/wandb/run-20230228_041452-kecs7b8p\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33maug_batch16_epoch24\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/textnet/MemeProject\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/textnet/MemeProject/runs/kecs7b8p\u001b[0m\n","***** Running training *****\n","  Num examples = 2776\n","  Num Epochs = 24\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4176\n","  Number of trainable parameters = 123859968\n","Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","{'loss': 1.6065, 'learning_rate': 4.742002063983488e-05, 'epoch': 2.87}\n"," 12% 500/4176 [02:18<16:38,  3.68it/s]***** Running Evaluation *****\n","  Num examples = 694\n","  Batch size = 16\n","\n","  0% 0/44 [00:00<?, ?it/s]\u001b[A\n","  7% 3/44 [00:00<00:02, 19.29it/s]\u001b[A\n"," 11% 5/44 [00:00<00:02, 15.45it/s]\u001b[A\n"," 16% 7/44 [00:00<00:02, 14.21it/s]\u001b[A\n"," 20% 9/44 [00:00<00:02, 13.64it/s]\u001b[A\n"," 25% 11/44 [00:00<00:02, 13.36it/s]\u001b[A\n"," 30% 13/44 [00:00<00:02, 13.16it/s]\u001b[A\n"," 34% 15/44 [00:01<00:02, 12.98it/s]\u001b[A\n"," 39% 17/44 [00:01<00:02, 12.88it/s]\u001b[A\n"," 43% 19/44 [00:01<00:01, 12.86it/s]\u001b[A\n"," 48% 21/44 [00:01<00:01, 12.89it/s]\u001b[A\n"," 52% 23/44 [00:01<00:01, 12.86it/s]\u001b[A\n"," 57% 25/44 [00:01<00:01, 12.84it/s]\u001b[A\n"," 61% 27/44 [00:02<00:01, 12.82it/s]\u001b[A\n"," 66% 29/44 [00:02<00:01, 12.82it/s]\u001b[A\n"," 70% 31/44 [00:02<00:01, 12.75it/s]\u001b[A\n"," 75% 33/44 [00:02<00:00, 12.83it/s]\u001b[A\n"," 80% 35/44 [00:02<00:00, 12.81it/s]\u001b[A\n"," 84% 37/44 [00:02<00:00, 12.81it/s]\u001b[A\n"," 89% 39/44 [00:02<00:00, 12.79it/s]\u001b[A\n"," 93% 41/44 [00:03<00:00, 12.80it/s]\u001b[A\n","                                      \n","\u001b[A{'eval_loss': 0.018154596909880638, 'eval_runtime': 3.4032, 'eval_samples_per_second': 203.924, 'eval_steps_per_second': 12.929, 'epoch': 2.87}\n"," 12% 500/4176 [02:22<16:38,  3.68it/s]\n","100% 44/44 [00:03<00:00, 12.79it/s]\u001b[A\n","{'loss': 0.0248, 'learning_rate': 4.097007223942209e-05, 'epoch': 5.75}\n"," 24% 1000/4176 [04:37<14:18,  3.70it/s]***** Running Evaluation *****\n","  Num examples = 694\n","  Batch size = 16\n","\n","  0% 0/44 [00:00<?, ?it/s]\u001b[A\n","  7% 3/44 [00:00<00:02, 19.24it/s]\u001b[A\n"," 11% 5/44 [00:00<00:02, 15.44it/s]\u001b[A\n"," 16% 7/44 [00:00<00:02, 14.23it/s]\u001b[A\n"," 20% 9/44 [00:00<00:02, 13.64it/s]\u001b[A\n"," 25% 11/44 [00:00<00:02, 13.34it/s]\u001b[A\n"," 30% 13/44 [00:00<00:02, 13.14it/s]\u001b[A\n"," 34% 15/44 [00:01<00:02, 13.05it/s]\u001b[A\n"," 39% 17/44 [00:01<00:02, 12.97it/s]\u001b[A\n"," 43% 19/44 [00:01<00:01, 12.93it/s]\u001b[A\n"," 48% 21/44 [00:01<00:01, 12.88it/s]\u001b[A\n"," 52% 23/44 [00:01<00:01, 12.69it/s]\u001b[A\n"," 57% 25/44 [00:01<00:01, 12.73it/s]\u001b[A\n"," 61% 27/44 [00:02<00:01, 12.75it/s]\u001b[A\n"," 66% 29/44 [00:02<00:01, 12.77it/s]\u001b[A\n"," 70% 31/44 [00:02<00:01, 12.78it/s]\u001b[A\n"," 75% 33/44 [00:02<00:00, 12.79it/s]\u001b[A\n"," 80% 35/44 [00:02<00:00, 12.79it/s]\u001b[A\n"," 84% 37/44 [00:02<00:00, 12.79it/s]\u001b[A\n"," 89% 39/44 [00:02<00:00, 12.80it/s]\u001b[A\n"," 93% 41/44 [00:03<00:00, 12.80it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.0218878835439682, 'eval_runtime': 3.4068, 'eval_samples_per_second': 203.707, 'eval_steps_per_second': 12.915, 'epoch': 5.75}\n"," 24% 1000/4176 [04:41<14:18,  3.70it/s]\n","100% 44/44 [00:03<00:00, 12.81it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/drive/MyDrive/MemeProject/model/pytorch/transfer/checkpoints/checkpoint-1000\n","Configuration saved in /content/drive/MyDrive/MemeProject/model/pytorch/transfer/checkpoints/checkpoint-1000/config.json\n","Configuration saved in /content/drive/MyDrive/MemeProject/model/pytorch/transfer/checkpoints/checkpoint-1000/generation_config.json\n","Model weights saved in /content/drive/MyDrive/MemeProject/model/pytorch/transfer/checkpoints/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/MemeProject/model/pytorch/transfer/checkpoints/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/MemeProject/model/pytorch/transfer/checkpoints/checkpoint-1000/special_tokens_map.json\n","{'loss': 0.0074, 'learning_rate': 3.452012383900929e-05, 'epoch': 8.62}\n"," 36% 1500/4176 [07:00<12:05,  3.69it/s]***** Running Evaluation *****\n","  Num examples = 694\n","  Batch size = 16\n","\n","  0% 0/44 [00:00<?, ?it/s]\u001b[A\n","  7% 3/44 [00:00<00:02, 19.15it/s]\u001b[A\n"," 11% 5/44 [00:00<00:02, 15.32it/s]\u001b[A\n"," 16% 7/44 [00:00<00:02, 14.10it/s]\u001b[A\n"," 20% 9/44 [00:00<00:02, 13.55it/s]\u001b[A\n"," 25% 11/44 [00:00<00:02, 13.25it/s]\u001b[A\n"," 30% 13/44 [00:00<00:02, 13.05it/s]\u001b[A\n"," 34% 15/44 [00:01<00:02, 12.93it/s]\u001b[A\n"," 39% 17/44 [00:01<00:02, 12.85it/s]\u001b[A\n"," 43% 19/44 [00:01<00:01, 12.79it/s]\u001b[A\n"," 48% 21/44 [00:01<00:01, 12.75it/s]\u001b[A\n"," 52% 23/44 [00:01<00:01, 12.73it/s]\u001b[A\n"," 57% 25/44 [00:01<00:01, 12.71it/s]\u001b[A\n"," 61% 27/44 [00:02<00:01, 12.71it/s]\u001b[A\n"," 66% 29/44 [00:02<00:01, 12.70it/s]\u001b[A\n"," 70% 31/44 [00:02<00:01, 12.71it/s]\u001b[A\n"," 75% 33/44 [00:02<00:00, 12.74it/s]\u001b[A\n"," 80% 35/44 [00:02<00:00, 12.74it/s]\u001b[A\n"," 84% 37/44 [00:02<00:00, 12.77it/s]\u001b[A\n"," 89% 39/44 [00:02<00:00, 12.79it/s]\u001b[A\n"," 93% 41/44 [00:03<00:00, 12.78it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.023899707943201065, 'eval_runtime': 3.4236, 'eval_samples_per_second': 202.711, 'eval_steps_per_second': 12.852, 'epoch': 8.62}\n"," 36% 1500/4176 [07:03<12:05,  3.69it/s]\n","100% 44/44 [00:03<00:00, 12.78it/s]\u001b[A\n","{'loss': 0.0029, 'learning_rate': 2.8070175438596492e-05, 'epoch': 11.49}\n"," 48% 2000/4176 [09:19<09:50,  3.69it/s]***** Running Evaluation *****\n","  Num examples = 694\n","  Batch size = 16\n","\n","  0% 0/44 [00:00<?, ?it/s]\u001b[A\n","  7% 3/44 [00:00<00:02, 19.26it/s]\u001b[A\n"," 11% 5/44 [00:00<00:02, 15.44it/s]\u001b[A\n"," 16% 7/44 [00:00<00:02, 14.22it/s]\u001b[A\n"," 20% 9/44 [00:00<00:02, 13.64it/s]\u001b[A\n"," 25% 11/44 [00:00<00:02, 13.33it/s]\u001b[A\n"," 30% 13/44 [00:00<00:02, 13.15it/s]\u001b[A\n"," 34% 15/44 [00:01<00:02, 13.00it/s]\u001b[A\n"," 39% 17/44 [00:01<00:02, 12.93it/s]\u001b[A\n"," 43% 19/44 [00:01<00:01, 12.87it/s]\u001b[A\n"," 48% 21/44 [00:01<00:01, 12.84it/s]\u001b[A\n"," 52% 23/44 [00:01<00:01, 12.83it/s]\u001b[A\n"," 57% 25/44 [00:01<00:01, 12.81it/s]\u001b[A\n"," 61% 27/44 [00:02<00:01, 12.79it/s]\u001b[A\n"," 66% 29/44 [00:02<00:01, 12.79it/s]\u001b[A\n"," 70% 31/44 [00:02<00:01, 12.79it/s]\u001b[A\n"," 75% 33/44 [00:02<00:00, 12.77it/s]\u001b[A\n"," 80% 35/44 [00:02<00:00, 12.74it/s]\u001b[A\n"," 84% 37/44 [00:02<00:00, 12.74it/s]\u001b[A\n"," 89% 39/44 [00:02<00:00, 12.75it/s]\u001b[A\n"," 93% 41/44 [00:03<00:00, 12.75it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.02417723275721073, 'eval_runtime': 3.4136, 'eval_samples_per_second': 203.303, 'eval_steps_per_second': 12.89, 'epoch': 11.49}\n"," 48% 2000/4176 [09:22<09:50,  3.69it/s]\n","100% 44/44 [00:03<00:00, 12.75it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/drive/MyDrive/MemeProject/model/pytorch/transfer/checkpoints/checkpoint-2000\n","Configuration saved in /content/drive/MyDrive/MemeProject/model/pytorch/transfer/checkpoints/checkpoint-2000/config.json\n","Configuration saved in /content/drive/MyDrive/MemeProject/model/pytorch/transfer/checkpoints/checkpoint-2000/generation_config.json\n","Model weights saved in /content/drive/MyDrive/MemeProject/model/pytorch/transfer/checkpoints/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/MemeProject/model/pytorch/transfer/checkpoints/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/MemeProject/model/pytorch/transfer/checkpoints/checkpoint-2000/special_tokens_map.json\n","{'loss': 0.0015, 'learning_rate': 2.1620227038183697e-05, 'epoch': 14.37}\n"," 60% 2500/4176 [11:42<07:36,  3.67it/s]***** Running Evaluation *****\n","  Num examples = 694\n","  Batch size = 16\n","\n","  0% 0/44 [00:00<?, ?it/s]\u001b[A\n","  7% 3/44 [00:00<00:02, 19.10it/s]\u001b[A\n"," 11% 5/44 [00:00<00:02, 15.35it/s]\u001b[A\n"," 16% 7/44 [00:00<00:02, 14.14it/s]\u001b[A\n"," 20% 9/44 [00:00<00:02, 13.58it/s]\u001b[A\n"," 25% 11/44 [00:00<00:02, 13.26it/s]\u001b[A\n"," 30% 13/44 [00:00<00:02, 13.09it/s]\u001b[A\n"," 34% 15/44 [00:01<00:02, 12.96it/s]\u001b[A\n"," 39% 17/44 [00:01<00:02, 12.87it/s]\u001b[A\n"," 43% 19/44 [00:01<00:01, 12.83it/s]\u001b[A\n"," 48% 21/44 [00:01<00:01, 12.80it/s]\u001b[A\n"," 52% 23/44 [00:01<00:01, 12.77it/s]\u001b[A\n"," 57% 25/44 [00:01<00:01, 12.75it/s]\u001b[A\n"," 61% 27/44 [00:02<00:01, 12.68it/s]\u001b[A\n"," 66% 29/44 [00:02<00:01, 12.56it/s]\u001b[A\n"," 70% 31/44 [00:02<00:01, 12.61it/s]\u001b[A\n"," 75% 33/44 [00:02<00:00, 12.62it/s]\u001b[A\n"," 80% 35/44 [00:02<00:00, 12.65it/s]\u001b[A\n"," 84% 37/44 [00:02<00:00, 12.62it/s]\u001b[A\n"," 89% 39/44 [00:02<00:00, 12.68it/s]\u001b[A\n"," 93% 41/44 [00:03<00:00, 12.65it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.023905949667096138, 'eval_runtime': 3.4324, 'eval_samples_per_second': 202.19, 'eval_steps_per_second': 12.819, 'epoch': 14.37}\n"," 60% 2500/4176 [11:45<07:36,  3.67it/s]\n","100% 44/44 [00:03<00:00, 12.70it/s]\u001b[A\n","{'loss': 0.001, 'learning_rate': 1.5170278637770899e-05, 'epoch': 17.24}\n"," 72% 3000/4176 [14:01<05:21,  3.66it/s]***** Running Evaluation *****\n","  Num examples = 694\n","  Batch size = 16\n","\n","  0% 0/44 [00:00<?, ?it/s]\u001b[A\n","  7% 3/44 [00:00<00:02, 19.22it/s]\u001b[A\n"," 11% 5/44 [00:00<00:02, 15.35it/s]\u001b[A\n"," 16% 7/44 [00:00<00:02, 14.12it/s]\u001b[A\n"," 20% 9/44 [00:00<00:02, 13.55it/s]\u001b[A\n"," 25% 11/44 [00:00<00:02, 13.22it/s]\u001b[A\n"," 30% 13/44 [00:00<00:02, 13.05it/s]\u001b[A\n"," 34% 15/44 [00:01<00:02, 12.91it/s]\u001b[A\n"," 39% 17/44 [00:01<00:02, 12.85it/s]\u001b[A\n"," 43% 19/44 [00:01<00:01, 12.80it/s]\u001b[A\n"," 48% 21/44 [00:01<00:01, 12.77it/s]\u001b[A\n"," 52% 23/44 [00:01<00:01, 12.76it/s]\u001b[A\n"," 57% 25/44 [00:01<00:01, 12.75it/s]\u001b[A\n"," 61% 27/44 [00:02<00:01, 12.73it/s]\u001b[A\n"," 66% 29/44 [00:02<00:01, 12.72it/s]\u001b[A\n"," 70% 31/44 [00:02<00:01, 12.72it/s]\u001b[A\n"," 75% 33/44 [00:02<00:00, 12.67it/s]\u001b[A\n"," 80% 35/44 [00:02<00:00, 12.75it/s]\u001b[A\n"," 84% 37/44 [00:02<00:00, 12.76it/s]\u001b[A\n"," 89% 39/44 [00:02<00:00, 12.75it/s]\u001b[A\n"," 93% 41/44 [00:03<00:00, 12.74it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.025425702333450317, 'eval_runtime': 3.4285, 'eval_samples_per_second': 202.424, 'eval_steps_per_second': 12.834, 'epoch': 17.24}\n"," 72% 3000/4176 [14:04<05:21,  3.66it/s]\n","100% 44/44 [00:03<00:00, 12.74it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/drive/MyDrive/MemeProject/model/pytorch/transfer/checkpoints/checkpoint-3000\n","Configuration saved in /content/drive/MyDrive/MemeProject/model/pytorch/transfer/checkpoints/checkpoint-3000/config.json\n","Configuration saved in /content/drive/MyDrive/MemeProject/model/pytorch/transfer/checkpoints/checkpoint-3000/generation_config.json\n","Model weights saved in /content/drive/MyDrive/MemeProject/model/pytorch/transfer/checkpoints/checkpoint-3000/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/MemeProject/model/pytorch/transfer/checkpoints/checkpoint-3000/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/MemeProject/model/pytorch/transfer/checkpoints/checkpoint-3000/special_tokens_map.json\n","{'loss': 0.0007, 'learning_rate': 8.7203302373581e-06, 'epoch': 20.11}\n"," 84% 3500/4176 [16:24<03:04,  3.67it/s]***** Running Evaluation *****\n","  Num examples = 694\n","  Batch size = 16\n","\n","  0% 0/44 [00:00<?, ?it/s]\u001b[A\n","  7% 3/44 [00:00<00:02, 19.14it/s]\u001b[A\n"," 11% 5/44 [00:00<00:02, 15.35it/s]\u001b[A\n"," 16% 7/44 [00:00<00:02, 14.04it/s]\u001b[A\n"," 20% 9/44 [00:00<00:02, 13.58it/s]\u001b[A\n"," 25% 11/44 [00:00<00:02, 13.27it/s]\u001b[A\n"," 30% 13/44 [00:00<00:02, 13.07it/s]\u001b[A\n"," 34% 15/44 [00:01<00:02, 12.95it/s]\u001b[A\n"," 39% 17/44 [00:01<00:02, 12.87it/s]\u001b[A\n"," 43% 19/44 [00:01<00:01, 12.82it/s]\u001b[A\n"," 48% 21/44 [00:01<00:01, 12.77it/s]\u001b[A\n"," 52% 23/44 [00:01<00:01, 12.75it/s]\u001b[A\n"," 57% 25/44 [00:01<00:01, 12.72it/s]\u001b[A\n"," 61% 27/44 [00:02<00:01, 12.71it/s]\u001b[A\n"," 66% 29/44 [00:02<00:01, 12.69it/s]\u001b[A\n"," 70% 31/44 [00:02<00:01, 12.68it/s]\u001b[A\n"," 75% 33/44 [00:02<00:00, 12.68it/s]\u001b[A\n"," 80% 35/44 [00:02<00:00, 12.68it/s]\u001b[A\n"," 84% 37/44 [00:02<00:00, 12.68it/s]\u001b[A\n"," 89% 39/44 [00:02<00:00, 12.67it/s]\u001b[A\n"," 93% 41/44 [00:03<00:00, 12.68it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.02532728761434555, 'eval_runtime': 3.4307, 'eval_samples_per_second': 202.294, 'eval_steps_per_second': 12.826, 'epoch': 20.11}\n"," 84% 3500/4176 [16:27<03:04,  3.67it/s]\n","100% 44/44 [00:03<00:00, 12.67it/s]\u001b[A\n","{'loss': 0.0005, 'learning_rate': 2.2703818369453045e-06, 'epoch': 22.99}\n"," 96% 4000/4176 [18:43<00:47,  3.68it/s]***** Running Evaluation *****\n","  Num examples = 694\n","  Batch size = 16\n","\n","  0% 0/44 [00:00<?, ?it/s]\u001b[A\n","  7% 3/44 [00:00<00:02, 19.13it/s]\u001b[A\n"," 11% 5/44 [00:00<00:02, 14.88it/s]\u001b[A\n"," 16% 7/44 [00:00<00:02, 14.28it/s]\u001b[A\n"," 20% 9/44 [00:00<00:02, 13.66it/s]\u001b[A\n"," 25% 11/44 [00:00<00:02, 13.33it/s]\u001b[A\n"," 30% 13/44 [00:00<00:02, 13.14it/s]\u001b[A\n"," 34% 15/44 [00:01<00:02, 13.00it/s]\u001b[A\n"," 39% 17/44 [00:01<00:02, 12.91it/s]\u001b[A\n"," 43% 19/44 [00:01<00:01, 12.86it/s]\u001b[A\n"," 48% 21/44 [00:01<00:01, 12.82it/s]\u001b[A\n"," 52% 23/44 [00:01<00:01, 12.80it/s]\u001b[A\n"," 57% 25/44 [00:01<00:01, 12.78it/s]\u001b[A\n"," 61% 27/44 [00:02<00:01, 12.76it/s]\u001b[A\n"," 66% 29/44 [00:02<00:01, 12.76it/s]\u001b[A\n"," 70% 31/44 [00:02<00:01, 12.76it/s]\u001b[A\n"," 75% 33/44 [00:02<00:00, 12.75it/s]\u001b[A\n"," 80% 35/44 [00:02<00:00, 12.75it/s]\u001b[A\n"," 84% 37/44 [00:02<00:00, 12.75it/s]\u001b[A\n"," 89% 39/44 [00:02<00:00, 12.75it/s]\u001b[A\n"," 93% 41/44 [00:03<00:00, 12.73it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.026036038994789124, 'eval_runtime': 3.4171, 'eval_samples_per_second': 203.094, 'eval_steps_per_second': 12.876, 'epoch': 22.99}\n"," 96% 4000/4176 [18:47<00:47,  3.68it/s]\n","100% 44/44 [00:03<00:00, 12.74it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/drive/MyDrive/MemeProject/model/pytorch/transfer/checkpoints/checkpoint-4000\n","Configuration saved in /content/drive/MyDrive/MemeProject/model/pytorch/transfer/checkpoints/checkpoint-4000/config.json\n","Configuration saved in /content/drive/MyDrive/MemeProject/model/pytorch/transfer/checkpoints/checkpoint-4000/generation_config.json\n","Model weights saved in /content/drive/MyDrive/MemeProject/model/pytorch/transfer/checkpoints/checkpoint-4000/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/MemeProject/model/pytorch/transfer/checkpoints/checkpoint-4000/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/MemeProject/model/pytorch/transfer/checkpoints/checkpoint-4000/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/MemeProject/model/pytorch/transfer/checkpoints/checkpoint-1000] due to args.save_total_limit\n","100% 4176/4176 [19:38<00:00,  4.22it/s]\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 1178.7996, 'train_samples_per_second': 56.519, 'train_steps_per_second': 3.543, 'train_loss': 0.19701538632634555, 'epoch': 24.0}\n","100% 4176/4176 [19:38<00:00,  3.54it/s]\n","Saving model checkpoint to /content/drive/MyDrive/MemeProject/model/pytorch/transfer/outputs\n","Configuration saved in /content/drive/MyDrive/MemeProject/model/pytorch/transfer/outputs/config.json\n","Configuration saved in /content/drive/MyDrive/MemeProject/model/pytorch/transfer/outputs/generation_config.json\n","Model weights saved in /content/drive/MyDrive/MemeProject/model/pytorch/transfer/outputs/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/MemeProject/model/pytorch/transfer/outputs/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/MemeProject/model/pytorch/transfer/outputs/special_tokens_map.json\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss â–â–„â–†â–†â–†â–‡â–‡â–ˆ\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime â–â–‚â–†â–ƒâ–ˆâ–‡â–ˆâ–„\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second â–ˆâ–‡â–ƒâ–…â–â–‚â–â–…\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second â–ˆâ–‡â–ƒâ–†â–â–‚â–â–…\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆ\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆ\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate â–ˆâ–‡â–†â–…â–„â–ƒâ–‚â–\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss â–ˆâ–â–â–â–â–â–â–\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos â–\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss â–\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime â–\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second â–\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second â–\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.02604\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 3.4171\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 203.094\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 12.876\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 24.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 4176\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.0005\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 1.015577044844544e+16\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.19702\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 1178.7996\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 56.519\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 3.543\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33maug_batch16_epoch24\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/textnet/MemeProject/runs/kecs7b8p\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230228_041452-kecs7b8p/logs\u001b[0m\n"]}]},{"cell_type":"markdown","source":["# Predict"],"metadata":{"id":"PfcKvvMeB78X"}},{"cell_type":"code","source":["import os\n","os.chdir('drive/MyDrive/MemeProject/src/pytorch/augmentation')"],"metadata":{"id":"4jT7p25KoLli"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python3 predict.py"],"metadata":{"id":"GBm7M5KyTEA6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hK1W8H5AyYYl"},"execution_count":null,"outputs":[]}]}