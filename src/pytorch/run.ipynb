{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","collapsed_sections":["PfcKvvMeB78X"],"authorship_tag":"ABX9TyMREsJxuBxEwO2TCV8VMhWo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"premium","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Install"],"metadata":{"id":"FN-Jm9IgyynN"}},{"cell_type":"code","source":["!pip3 install transformers\n","!pip3 install datasets\n","!pip3 install evaluate\n","!pip3 install torch"],"metadata":{"id":"cEWPHH3o6CZk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip3 install pshmodule\n","!pip3 install wandb\n","!pip3 install pandas==1.5.0\n","!pip3 install pickle5\n","!pip3 install hydra-core"],"metadata":{"id":"yzC54sGWPm3t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"qqomZDQhz488","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675331720433,"user_tz":-540,"elapsed":30037,"user":{"displayName":"ë°•ì„±í™˜","userId":"16608640204304641951"}},"outputId":"3b12d7aa-2be7-4643-d0cb-b10f1f20e837"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# GPU Check"],"metadata":{"id":"Fiap6RttXJza"}},{"cell_type":"code","source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LXqlHhTzXz49","executionInfo":{"status":"ok","timestamp":1675331726517,"user_tz":-540,"elapsed":1075,"user":{"displayName":"ë°•ì„±í™˜","userId":"16608640204304641951"}},"outputId":"105def11-c93c-40e1-8fe6-b0c73d34ab79"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Feb  2 09:55:25 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   34C    P0    50W / 400W |      0MiB / 40960MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lbRds2i6X5Hl","executionInfo":{"status":"ok","timestamp":1675331727186,"user_tz":-540,"elapsed":671,"user":{"displayName":"ë°•ì„±í™˜","userId":"16608640204304641951"}},"outputId":"da4c010b-1105-490e-9857-a4972eb4f8aa"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Your runtime has 89.6 gigabytes of available RAM\n","\n","You are using a high-RAM runtime!\n"]}]},{"cell_type":"markdown","source":["# Train"],"metadata":{"id":"1D-Ls5bAXRVN"}},{"cell_type":"code","source":["import os\n","os.chdir('/content/drive/MyDrive/MemeProject/src/pytorch/')\n","\n","!python3 train.py"],"metadata":{"id":"ACD6i5ZdXJGa","colab":{"base_uri":"https://localhost:8080/"},"outputId":"beb9ebe8-08b8-4a53-bc40-e2265ceda4df","executionInfo":{"status":"ok","timestamp":1675331874059,"user_tz":-540,"elapsed":143000,"user":{"displayName":"ë°•ì„±í™˜","userId":"16608640204304641951"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-02-02 09:55:34.305566: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","train.py:17: UserWarning: \n","The version_base parameter is not specified.\n","Please specify a compatability version level, or None.\n","Will assume defaults for version 1.1\n","  @hydra.main(config_path=\"./\", config_name=\"config\")\n","/usr/local/lib/python3.8/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n","See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n","  ret = run_job(\n","Downloading (â€¦)okenizer_config.json: 100% 288/288 [00:00<00:00, 47.6kB/s]\n","Downloading (â€¦)lve/main/config.json: 100% 504/504 [00:00<00:00, 209kB/s]\n","Downloading (â€¦)solve/main/vocab.txt: 100% 450k/450k [00:01<00:00, 403kB/s]\n","Downloading (â€¦)cial_tokens_map.json: 100% 124/124 [00:00<00:00, 50.3kB/s]\n","Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-5a186262c7a16c02/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n","Downloading data files: 100% 1/1 [00:00<00:00, 2949.58it/s]\n","Extracting data files: 100% 1/1 [00:00<00:00, 198.13it/s]\n","Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-5a186262c7a16c02/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n","100% 2/2 [00:00<00:00, 740.45it/s]\n","#0:   0% 0/15 [00:00<?, ?ba/s]\n","#1:   0% 0/15 [00:00<?, ?ba/s]\u001b[A\n","\n","#2:   0% 0/15 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","#0:  20% 3/15 [00:00<00:00, 29.95ba/s]\n","#1:  27% 4/15 [00:00<00:00, 36.14ba/s]\u001b[A\n","\n","#2:  33% 5/15 [00:00<00:00, 46.75ba/s]\u001b[A\u001b[A\n","\n","\n","#0:  67% 10/15 [00:00<00:00, 53.27ba/s]\n","#1:  73% 11/15 [00:00<00:00, 55.20ba/s]\u001b[A\n","\n","#2:  87% 13/15 [00:00<00:00, 61.01ba/s]\u001b[A\u001b[A\n","\n","\n","#0: 100% 15/15 [00:00<00:00, 57.17ba/s]\n","#2: 100% 15/15 [00:00<00:00, 62.48ba/s]\n","#3: 100% 15/15 [00:00<00:00, 67.07ba/s]\n","#1: 100% 15/15 [00:00<00:00, 58.16ba/s]\n","#0:   0% 0/4 [00:00<?, ?ba/s]\n","#1:   0% 0/4 [00:00<?, ?ba/s]\u001b[A\n","\n","#2:   0% 0/4 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","#0: 100% 4/4 [00:00<00:00, 56.74ba/s]\n","#1: 100% 4/4 [00:00<00:00, 57.65ba/s]\n","#2: 100% 4/4 [00:00<00:00, 58.18ba/s]\n","#3: 100% 4/4 [00:00<00:00, 57.78ba/s]\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/MemeProject/src/pytorch/outputs/2023-02-02/09-55-41/wandb/run-20230202_095713-a7fobxzm\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbatch128_epoch20_class154\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/textnet/MemeProject\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/textnet/MemeProject/runs/a7fobxzm\u001b[0m\n","Downloading (â€¦)\"pytorch_model.bin\";: 100% 511M/511M [00:10<00:00, 47.6MB/s]\n","Some weights of the model checkpoint at beomi/KcELECTRA-base-v2022 were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight']\n","- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base-v2022 and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Downloading builder script: 100% 6.77k/6.77k [00:00<00:00, 5.32MB/s]\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 7320\n","  Num Epochs = 20\n","  Instantaneous batch size per device = 128\n","  Total train batch size (w. parallel, distributed & accumulation) = 128\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1160\n","  Number of trainable parameters = 127895194\n","Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  0% 0/1160 [00:00<?, ?it/s]Error executing job with overrides: []\n","Traceback (most recent call last):\n","  File \"train.py\", line 92, in main\n","    trainer.train()\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 1543, in train\n","    return inner_training_loop(\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 1791, in _inner_training_loop\n","    tr_loss_step = self.training_step(model, inputs)\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 2539, in training_step\n","    loss = self.compute_loss(model, inputs)\n","  File \"train.py\", line 70, in compute_loss\n","    loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py\", line 1174, in forward\n","    return F.cross_entropy(input, target, weight=self.weight,\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\", line 3026, in cross_entropy\n","    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n","RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument weight in method wrapper_nll_loss_forward)\n","\n","Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mbatch128_epoch20_class154\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/textnet/MemeProject/runs/a7fobxzm\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230202_095713-a7fobxzm/logs\u001b[0m\n","  0% 0/1160 [00:10<?, ?it/s]\n"]}]},{"cell_type":"markdown","source":["# Predict"],"metadata":{"id":"PfcKvvMeB78X"}},{"cell_type":"code","source":["import os\n","os.chdir('/content/drive/MyDrive/MemeProject/src/pytorch/')\n","\n","!python3 predict.py"],"metadata":{"id":"4jT7p25KoLli","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674784186570,"user_tz":-540,"elapsed":39885,"user":{"displayName":"ë°•ì„±í™˜","userId":"16608640204304641951"}},"outputId":"0ecf2b41-b8c0-4f2e-8702-a0a62076d5de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-01-27 01:49:11.642004: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","predict.py:11: UserWarning: \n","The version_base parameter is not specified.\n","Please specify a compatability version level, or None.\n","Will assume defaults for version 1.1\n","  @hydra.main(config_path=\"./\", config_name=\"config\")\n","/usr/local/lib/python3.8/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n","See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n","  ret = run_job(\n","Downloading (â€¦)okenizer_config.json: 100% 288/288 [00:00<00:00, 46.5kB/s]\n","Downloading (â€¦)lve/main/config.json: 100% 504/504 [00:00<00:00, 202kB/s]\n","Downloading (â€¦)solve/main/vocab.txt: 100% 450k/450k [00:00<00:00, 1.05MB/s]\n","Downloading (â€¦)cial_tokens_map.json: 100% 124/124 [00:00<00:00, 48.1kB/s]\n","tokenizer loading done!\n","model loading done!\n","predict : 459\n","extension : .pickle\n","Loaded 181704 records from /content/drive/MyDrive/MemeProject/data/augmentation/origin_ref.pickle\n","u    íƒœë¯¼... ë¹¨ë¦¬ ìŒì› ë‚´ì¤¬ìœ¼ë©´ ì¢‹ê² ë”°ì•„!!\n","Name: 68208, dtype: object\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"GBm7M5KyTEA6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3VZtNtLchrEr"},"execution_count":null,"outputs":[]}]}