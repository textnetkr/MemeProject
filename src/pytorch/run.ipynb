{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyOIFCK6ZF5cMRR6/6qlqud1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"premium","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Install"],"metadata":{"id":"FN-Jm9IgyynN"}},{"cell_type":"code","source":["!pip3 install hydra-core"],"metadata":{"id":"g376Z7kty0g5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip3 install transformers"],"metadata":{"id":"cc96tadMzKCR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip3 install datasets"],"metadata":{"id":"BZ8RWnMHrtVA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install evaluate"],"metadata":{"id":"ROJqqGRVClsg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip3 install pshmodule"],"metadata":{"id":"yzC54sGWPm3t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip3 install wandb"],"metadata":{"id":"ElwnbM7TWVCJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"qqomZDQhz488","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674639967185,"user_tz":-540,"elapsed":21507,"user":{"displayName":"박성환","userId":"16608640204304641951"}},"outputId":"44c9b77d-07ba-4c13-e7e6-7049cea4b0a6"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# GPU Check"],"metadata":{"id":"Fiap6RttXJza"}},{"cell_type":"code","source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LXqlHhTzXz49","executionInfo":{"status":"ok","timestamp":1674634257969,"user_tz":-540,"elapsed":1516,"user":{"displayName":"박성환","userId":"16608640204304641951"}},"outputId":"71458a1b-da9a-4014-ca58-1ccdc9b32bfc"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Jan 25 08:10:56 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   32C    P0    48W / 400W |      0MiB / 40960MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lbRds2i6X5Hl","executionInfo":{"status":"ok","timestamp":1674634257969,"user_tz":-540,"elapsed":3,"user":{"displayName":"박성환","userId":"16608640204304641951"}},"outputId":"a75f5c6a-4b1b-420f-b7c0-27c29757290b"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Your runtime has 89.6 gigabytes of available RAM\n","\n","You are using a high-RAM runtime!\n"]}]},{"cell_type":"markdown","source":["# Run"],"metadata":{"id":"1D-Ls5bAXRVN"}},{"cell_type":"code","source":["import os\n","os.chdir('/content/drive/MyDrive/MemeProject/src/pytorch/')\n","\n","!python3 train.py"],"metadata":{"id":"ACD6i5ZdXJGa","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e2aaf50b-8cbd-4da0-b351-09493f1c5451","executionInfo":{"status":"ok","timestamp":1674640135592,"user_tz":-540,"elapsed":151725,"user":{"displayName":"박성환","userId":"16608640204304641951"}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-01-25 09:46:26.096650: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","train.py:15: UserWarning: \n","The version_base parameter is not specified.\n","Please specify a compatability version level, or None.\n","Will assume defaults for version 1.1\n","  @hydra.main(config_path=\"./\", config_name=\"config\")\n","/usr/local/lib/python3.8/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n","See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n","  ret = run_job(\n","Downloading: 100% 288/288 [00:00<00:00, 345kB/s]\n","Downloading: 100% 504/504 [00:00<00:00, 605kB/s]\n","Downloading: 100% 450k/450k [00:00<00:00, 3.86MB/s]\n","Downloading: 100% 124/124 [00:00<00:00, 150kB/s]\n","Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-86767ae1177a0623/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n","Downloading data files: 100% 1/1 [00:00<00:00, 2576.35it/s]\n","Extracting data files: 100% 1/1 [00:00<00:00, 177.34it/s]\n","Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-86767ae1177a0623/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n","100% 2/2 [00:00<00:00, 720.24it/s]\n","#0:   0% 0/15 [00:00<?, ?ba/s]\n","#1:   0% 0/15 [00:00<?, ?ba/s]\u001b[A\n","\n","#2:   0% 0/15 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","#0:  20% 3/15 [00:00<00:00, 28.84ba/s]\n","#1:  27% 4/15 [00:00<00:00, 35.24ba/s]\u001b[A\n","\n","#2:  27% 4/15 [00:00<00:00, 38.60ba/s]\u001b[A\u001b[A\n","\n","\n","#0:  53% 8/15 [00:00<00:00, 40.90ba/s]\n","#1:  67% 10/15 [00:00<00:00, 48.49ba/s]\u001b[A\n","\n","#2:  67% 10/15 [00:00<00:00, 47.34ba/s]\u001b[A\u001b[A\n","\n","\n","#1: 100% 15/15 [00:00<00:00, 51.81ba/s]\n","#2: 100% 15/15 [00:00<00:00, 51.10ba/s]\n","#3: 100% 15/15 [00:00<00:00, 52.79ba/s]\n","#0: 100% 15/15 [00:00<00:00, 46.52ba/s]\n","#0:   0% 0/4 [00:00<?, ?ba/s]\n","#1:   0% 0/4 [00:00<?, ?ba/s]\u001b[A\n","\n","#2:   0% 0/4 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","#0: 100% 4/4 [00:00<00:00, 48.06ba/s]\n","#1: 100% 4/4 [00:00<00:00, 48.79ba/s]\n","#2: 100% 4/4 [00:00<00:00, 48.58ba/s]\n","#3: 100% 4/4 [00:00<00:00, 49.90ba/s]\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/MemeProject/src/pytorch/outputs/2023-01-25/09-46-32/wandb/run-20230125_094648-8j7o8xh3\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbatch128_epoch60\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/textnet/MemeProject\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/textnet/MemeProject/runs/8j7o8xh3\u001b[0m\n","Downloading: 100% 511M/511M [00:05<00:00, 98.7MB/s]\n","Some weights of the model checkpoint at beomi/KcELECTRA-base-v2022 were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias']\n","- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base-v2022 and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 7316\n","  Num Epochs = 60\n","  Instantaneous batch size per device = 128\n","  Total train batch size (w. parallel, distributed & accumulation) = 128\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3480\n","  Number of trainable parameters = 128949493\n","Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  2% 58/3480 [01:19<56:36,  1.01it/s]  ***** Running Evaluation *****\n","  Num examples = 1829\n","  Batch size = 128\n","\n","  0% 0/15 [00:00<?, ?it/s]\u001b[A\n"," 13% 2/15 [00:00<00:03,  3.97it/s]\u001b[A\n"," 20% 3/15 [00:01<00:04,  2.80it/s]\u001b[A\n"," 27% 4/15 [00:01<00:04,  2.43it/s]\u001b[A\n"," 33% 5/15 [00:02<00:04,  2.24it/s]\u001b[A\n"," 40% 6/15 [00:02<00:04,  2.15it/s]\u001b[A\n"," 47% 7/15 [00:03<00:03,  2.09it/s]\u001b[A\n"," 53% 8/15 [00:03<00:03,  2.05it/s]\u001b[A\n"," 60% 9/15 [00:04<00:02,  2.03it/s]\u001b[A\n"," 67% 10/15 [00:04<00:02,  2.02it/s]\u001b[A\n"," 73% 11/15 [00:05<00:01,  2.00it/s]\u001b[A\n"," 80% 12/15 [00:05<00:01,  2.00it/s]\u001b[A\n"," 87% 13/15 [00:06<00:01,  1.98it/s]\u001b[A\n"," 93% 14/15 [00:06<00:00,  1.98it/s]\u001b[A\n","100% 15/15 [00:06<00:00,  2.50it/s]\u001b[A\n","\n","Downloading builder script: 100% 6.77k/6.77k [00:00<00:00, 4.54MB/s]\n","pred : [1074 1074 1074 ... 1074 1074 1074]\n","labels : [1449, 1424, 1399, 1248, 1461, 1386, 1350, 1388, 1306, 1415, 1519, 1511, 1419, 1409, 1285, 1408, 1231, 1383, 1254, 1508, 1329, 1262, 1492, 1519, 1237, 1488, 1459, 1250, 1242, 1282, 1359, 1393, 1491, 1335, 1277, 1485, 1280, 1259, 1464, 1426, 1312, 1241, 1312, 1331, 1401, 1505, 1223, 1294, 1491, 1433, 1293, 1366, 1231, 1464, 1484, 1521, 1460, 1338, 1339, 1358, 1475, 1473, 1354, 1426, 1222, 1303, 1281, 1431, 1261, 1347, 1310, 1447, 1334, 1483, 1232, 1391, 1280, 1514, 1260, 1339, 1414, 1310, 1515, 1366, 1225, 1338, 1275, 1311, 1421, 1502, 1470, 1432, 1390, 1430, 1381, 1478, 1298, 1236, 1345, 1311, 1382, 1251, 1313, 1457, 1523, 1245, 1239, 1409, 1363, 1513, 1273, 1266, 1381, 1345, 1371, 1261, 1474, 1322, 1460, 1357, 1412, 1484, 1341, 1309, 1486, 1477, 1370, 1437, 1384, 1321, 1270, 1504, 1420, 1407, 1394, 1435, 1453, 1428, 1509, 1377, 1291, 1259, 1471, 1493, 1403, 1264, 1486, 1369, 1292, 1306, 1420, 1445, 1224, 1429, 1322, 1468, 1322, 1446, 1468, 1238, 1393, 1507, 1436, 1319, 1520, 1451, 1338, 1254, 1379, 1378, 1435, 1431, 1253, 1368, 1427, 1373, 1223, 1508, 1277, 1295, 1461, 1317, 1284, 1509, 1383, 1515, 1443, 1266, 1438, 1487, 1362, 1390, 1324, 1406, 1265, 1270, 1336, 1518, 1406, 1238, 1279, 1402, 1225, 1308, 1400, 1362, 1469, 1226, 1488, 1394, 1387, 1295, 1313, 1224, 1402, 1506, 1447, 1332, 1290, 1501, 1395, 1487, 1376, 1492, 1409, 1293, 1241, 1355, 1233, 1243, 1322, 1267, 1269, 1462, 1238, 1492, 1302, 1400, 1296, 1487, 1324, 1428, 1371, 1272, 1384, 1234, 1452, 1343, 1411, 1276, 1236, 1247, 1456, 1353, 1244, 1274, 1332, 1458, 1257, 1457, 1451, 1393, 1327, 1375, 1447, 1406, 1357, 1411, 1231, 1236, 1490, 1517, 1475, 1269, 1359, 1434, 1246, 1310, 1472, 1445, 1255, 1321, 1459, 1518, 1302, 1295, 1516, 1340, 1485, 1389, 1350, 1347, 1421, 1324, 1259, 1512, 1495, 1517, 1333, 1473, 1411, 1419, 1438, 1277, 1364, 1446, 1482, 1301, 1494, 1317, 1386, 1524, 1454, 1388, 1266, 1422, 1220, 1362, 1376, 1420, 1413, 1243, 1442, 1319, 1297, 1238, 1249, 1478, 1437, 1483, 1370, 1234, 1220, 1253, 1503, 1328, 1503, 1363, 1351, 1320, 1297, 1448, 1465, 1361, 1351, 1233, 1326, 1425, 1326, 1314, 1505, 1308, 1402, 1351, 1510, 1326, 1469, 1481, 1503, 1242, 1429, 1226, 1315, 1332, 1433, 1300, 1284, 1452, 1325, 1331, 1399, 1290, 1299, 1228, 1436, 1222, 1478, 1367, 1337, 1481, 1400, 1221, 1439, 1347, 1383, 1408, 1473, 1516, 1475, 1325, 1331, 1339, 1240, 1229, 1256, 1421, 1429, 1441, 1303, 1454, 1318, 1369, 1251, 1379, 1365, 1476, 1435, 1510, 1484, 1458, 1451, 1430, 1279, 1489, 1301, 1307, 1499, 1328, 1343, 1225, 1353, 1496, 1397, 1375, 1500, 1272, 1467, 1436, 1499, 1350, 1380, 1410, 1513, 1379, 1472, 1366, 1427, 1370, 1253, 1514, 1285, 1331, 1367, 1327, 1444, 1472, 1309, 1488, 1404, 1490, 1281, 1511, 1258, 1244, 1367, 1321, 1389, 1297, 1386, 1356, 1522, 1290, 1438, 1515, 1401, 1257, 1438, 1244, 1252, 1246, 1334, 1312, 1342, 1520, 1284, 1502, 1357, 1238, 1422, 1446, 1253, 1447, 1377, 1417, 1269, 1301, 1456, 1409, 1521, 1449, 1488, 1353, 1431, 1401, 1387, 1448, 1267, 1256, 1470, 1448, 1387, 1343, 1264, 1302, 1490, 1329, 1452, 1406, 1417, 1251, 1419, 1374, 1448, 1385, 1405, 1475, 1418, 1365, 1468, 1361, 1477, 1361, 1239, 1381, 1266, 1453, 1332, 1468, 1369, 1340, 1250, 1300, 1391, 1412, 1375, 1482, 1347, 1372, 1471, 1286, 1391, 1506, 1496, 1297, 1373, 1515, 1501, 1385, 1336, 1388, 1444, 1364, 1325, 1423, 1287, 1327, 1371, 1479, 1267, 1352, 1405, 1480, 1246, 1480, 1283, 1425, 1423, 1275, 1291, 1440, 1443, 1415, 1255, 1340, 1470, 1233, 1273, 1497, 1381, 1497, 1307, 1384, 1300, 1473, 1502, 1462, 1490, 1311, 1364, 1420, 1372, 1471, 1482, 1406, 1323, 1223, 1349, 1308, 1440, 1490, 1371, 1306, 1261, 1479, 1357, 1307, 1343, 1442, 1346, 1281, 1281, 1387, 1246, 1349, 1389, 1312, 1470, 1340, 1354, 1225, 1284, 1222, 1469, 1476, 1298, 1314, 1405, 1293, 1385, 1294, 1286, 1254, 1316, 1309, 1303, 1254, 1444, 1477, 1265, 1398, 1332, 1359, 1368, 1489, 1241, 1514, 1450, 1296, 1500, 1410, 1485, 1271, 1489, 1473, 1330, 1282, 1294, 1372, 1440, 1234, 1434, 1410, 1461, 1440, 1344, 1302, 1394, 1476, 1227, 1446, 1317, 1373, 1467, 1384, 1254, 1223, 1354, 1421, 1320, 1440, 1345, 1399, 1223, 1286, 1287, 1455, 1432, 1423, 1439, 1367, 1479, 1481, 1243, 1410, 1249, 1524, 1307, 1441, 1522, 1465, 1241, 1389, 1316, 1287, 1249, 1332, 1471, 1331, 1327, 1524, 1507, 1281, 1247, 1399, 1374, 1328, 1379, 1521, 1491, 1521, 1450, 1464, 1386, 1289, 1349, 1354, 1420, 1342, 1257, 1289, 1257, 1348, 1323, 1242, 1358, 1230, 1247, 1457, 1458, 1292, 1361, 1295, 1226, 1505, 1458, 1243, 1248, 1271, 1423, 1252, 1509, 1478, 1486, 1308, 1281, 1220, 1492, 1261, 1292, 1334, 1276, 1355, 1429, 1304, 1339, 1252, 1466, 1295, 1272, 1484, 1458, 1221, 1255, 1268, 1395, 1418, 1289, 1309, 1300, 1453, 1434, 1325, 1355, 1451, 1472, 1474, 1455, 1271, 1323, 1453, 1498, 1291, 1477, 1396, 1345, 1503, 1450, 1385, 1447, 1232, 1491, 1269, 1402, 1316, 1510, 1289, 1433, 1482, 1486, 1245, 1448, 1375, 1345, 1380, 1271, 1239, 1226, 1351, 1464, 1343, 1524, 1408, 1279, 1335, 1255, 1248, 1318, 1455, 1223, 1270, 1299, 1288, 1299, 1517, 1435, 1316, 1337, 1439, 1501, 1348, 1484, 1466, 1505, 1252, 1262, 1344, 1411, 1471, 1498, 1286, 1463, 1333, 1403, 1273, 1230, 1381, 1284, 1442, 1523, 1469, 1304, 1518, 1445, 1291, 1515, 1409, 1228, 1360, 1318, 1424, 1335, 1403, 1314, 1236, 1502, 1397, 1416, 1252, 1244, 1319, 1350, 1496, 1477, 1483, 1454, 1486, 1257, 1235, 1393, 1378, 1390, 1307, 1305, 1414, 1249, 1237, 1521, 1329, 1263, 1480, 1387, 1249, 1265, 1380, 1442, 1493, 1466, 1487, 1398, 1296, 1519, 1464, 1229, 1397, 1289, 1512, 1258, 1366, 1464, 1240, 1327, 1346, 1435, 1410, 1450, 1457, 1355, 1333, 1222, 1246, 1462, 1353, 1231, 1417, 1430, 1446, 1245, 1482, 1304, 1442, 1377, 1315, 1369, 1434, 1388, 1222, 1421, 1483, 1403, 1286, 1312, 1426, 1336, 1319, 1360, 1426, 1382, 1472, 1303, 1304, 1294, 1455, 1282, 1236, 1436, 1221, 1373, 1288, 1315, 1227, 1278, 1443, 1409, 1262, 1406, 1260, 1356, 1501, 1224, 1428, 1338, 1299, 1461, 1222, 1311, 1263, 1441, 1336, 1511, 1432, 1522, 1383, 1513, 1279, 1396, 1230, 1237, 1306, 1454, 1505, 1302, 1500, 1264, 1441, 1412, 1226, 1493, 1511, 1478, 1348, 1384, 1317, 1407, 1324, 1228, 1479, 1259, 1429, 1459, 1358, 1443, 1503, 1362, 1380, 1315, 1394, 1390, 1494, 1378, 1380, 1255, 1357, 1352, 1512, 1504, 1283, 1510, 1311, 1457, 1278, 1345, 1290, 1221, 1422, 1507, 1413, 1323, 1265, 1228, 1356, 1232, 1404, 1408, 1394, 1405, 1291, 1450, 1509, 1330, 1518, 1469, 1296, 1484, 1305, 1328, 1490, 1329, 1305, 1238, 1268, 1475, 1242, 1233, 1236, 1351, 1333, 1397, 1418, 1385, 1221, 1452, 1488, 1330, 1458, 1411, 1285, 1454, 1304, 1436, 1337, 1498, 1379, 1386, 1389, 1459, 1441, 1474, 1368, 1365, 1376, 1486, 1427, 1277, 1432, 1234, 1258, 1462, 1297, 1377, 1364, 1362, 1272, 1272, 1465, 1357, 1352, 1264, 1519, 1517, 1491, 1450, 1400, 1434, 1390, 1348, 1456, 1516, 1433, 1404, 1283, 1504, 1395, 1498, 1395, 1301, 1308, 1334, 1285, 1340, 1353, 1230, 1483, 1449, 1460, 1310, 1495, 1322, 1283, 1309, 1258, 1524, 1282, 1499, 1285, 1227, 1430, 1227, 1364, 1341, 1276, 1520, 1454, 1463, 1392, 1495, 1514, 1324, 1259, 1280, 1372, 1230, 1356, 1373, 1497, 1505, 1523, 1456, 1242, 1517, 1405, 1356, 1389, 1456, 1349, 1476, 1245, 1314, 1423, 1246, 1433, 1472, 1245, 1483, 1274, 1500, 1280, 1244, 1463, 1342, 1344, 1402, 1359, 1342, 1424, 1231, 1335, 1392, 1318, 1436, 1240, 1278, 1360, 1404, 1348, 1254, 1267, 1250, 1419, 1391, 1365, 1460, 1242, 1294, 1398, 1428, 1408, 1273, 1419, 1496, 1415, 1489, 1360, 1393, 1453, 1414, 1423, 1463, 1494, 1226, 1473, 1415, 1487, 1287, 1466, 1392, 1382, 1288, 1347, 1310, 1305, 1234, 1344, 1301, 1328, 1417, 1422, 1248, 1323, 1256, 1371, 1337, 1369, 1346, 1416, 1413, 1292, 1286, 1282, 1342, 1235, 1374, 1482, 1325, 1263, 1418, 1427, 1290, 1330, 1239, 1494, 1306, 1287, 1268, 1516, 1401, 1416, 1445, 1234, 1352, 1315, 1392, 1460, 1422, 1369, 1247, 1358, 1500, 1278, 1522, 1509, 1256, 1463, 1480, 1221, 1337, 1507, 1487, 1405, 1279, 1469, 1233, 1290, 1321, 1370, 1395, 1243, 1341, 1367, 1300, 1511, 1437, 1346, 1499, 1432, 1274, 1276, 1437, 1311, 1320, 1228, 1344, 1252, 1376, 1391, 1256, 1256, 1241, 1517, 1492, 1392, 1359, 1412, 1433, 1326, 1479, 1413, 1416, 1336, 1310, 1237, 1280, 1354, 1467, 1404, 1224, 1335, 1420, 1485, 1305, 1314, 1260, 1388, 1497, 1288, 1268, 1339, 1513, 1321, 1508, 1297, 1462, 1359, 1348, 1520, 1452, 1476, 1461, 1307, 1265, 1439, 1230, 1395, 1429, 1274, 1427, 1378, 1463, 1303, 1280, 1428, 1247, 1470, 1267, 1396, 1523, 1247, 1276, 1327, 1457, 1299, 1449, 1378, 1375, 1293, 1481, 1508, 1361, 1319, 1336, 1514, 1496, 1370, 1499, 1465, 1365, 1294, 1235, 1312, 1468, 1500, 1430, 1335, 1418, 1376, 1501, 1262, 1342, 1273, 1377, 1346, 1438, 1445, 1362, 1522, 1445, 1367, 1277, 1270, 1425, 1403, 1278, 1346, 1298, 1380, 1449, 1347, 1260, 1377, 1284, 1368, 1360, 1323, 1493, 1519, 1493, 1240, 1303, 1248, 1355, 1338, 1382, 1235, 1520, 1296, 1410, 1270, 1235, 1431, 1416, 1319, 1459, 1237, 1349, 1467, 1501, 1407, 1502, 1439, 1444, 1225, 1282, 1511, 1349, 1480, 1425, 1427, 1338, 1250, 1398, 1255, 1320, 1299, 1309, 1479, 1321, 1477, 1394, 1350, 1360, 1506, 1313, 1249, 1398, 1387, 1437, 1453, 1354, 1413, 1350, 1451, 1504, 1507, 1428, 1523, 1407, 1266, 1424, 1441, 1414, 1315, 1300, 1401, 1316, 1275, 1363, 1313, 1263, 1258, 1232, 1263, 1430, 1292, 1278, 1268, 1414, 1407, 1404, 1293, 1516, 1241, 1419, 1425, 1508, 1509, 1353, 1497, 1375, 1237, 1451, 1399, 1384, 1370, 1326, 1229, 1288, 1271, 1283, 1231, 1251, 1329, 1471, 1393, 1424, 1522, 1388, 1398, 1318, 1440, 1417, 1295, 1494, 1358, 1261, 1317, 1411, 1412, 1507, 1499, 1468, 1417, 1337, 1232, 1515, 1506, 1313, 1374, 1227, 1330, 1275, 1262, 1356, 1325, 1476, 1461, 1251, 1495, 1421, 1401, 1313, 1413, 1467, 1259, 1220, 1442, 1521, 1493, 1466, 1277, 1495, 1271, 1407, 1363, 1275, 1391, 1250, 1324, 1480, 1474, 1386, 1425, 1306, 1275, 1262, 1519, 1383, 1298, 1251, 1363, 1431, 1518, 1448, 1334, 1496, 1220, 1424, 1253, 1498, 1399, 1266, 1412, 1287, 1481, 1244, 1439, 1514, 1318, 1366, 1224, 1233, 1475, 1352, 1504, 1444, 1467, 1466, 1308, 1276, 1373, 1361, 1372, 1263, 1285, 1371, 1510, 1355, 1465, 1474, 1270, 1396, 1248, 1400, 1403, 1229, 1291, 1272, 1326, 1229, 1265, 1512, 1422, 1268, 1239, 1283, 1434, 1296, 1513, 1298, 1292, 1446, 1341, 1383, 1264, 1488, 1516, 1293, 1385, 1489, 1512, 1435, 1253, 1502, 1443, 1400, 1379, 1305, 1358, 1489, 1443, 1510, 1261, 1416, 1462, 1402, 1267, 1390, 1460, 1524, 1239, 1513, 1227, 1452, 1273, 1392, 1481, 1497, 1437, 1224, 1320, 1465, 1301, 1328, 1426, 1455, 1372, 1229, 1368, 1258, 1363, 1447, 1351, 1397, 1494, 1333, 1504, 1257, 1334, 1470, 1302, 1320, 1382, 1508, 1382, 1415, 1376, 1264, 1314, 1245, 1364, 1459, 1366, 1381, 1397, 1225, 1343, 1260, 1485, 1374, 1288, 1378, 1498, 1339, 1374, 1322, 1365, 1352, 1344, 1250, 1232, 1331, 1432, 1260, 1269, 1414, 1431, 1240, 1341, 1408, 1341, 1523, 1444, 1269, 1289, 1330, 1333, 1240, 1478, 1506, 1474, 1396, 1243, 1438, 1396, 1317, 1279, 1456, 1492, 1485, 1495, 1329, 1520, 1228, 1506, 1426, 1274, 1304, 1503, 1235, 1512, 1340, 1455, 1449, 1274, 1316, 1418, 1368, 1298, 1491, 1415, 1518]\n","\n","{'eval_loss': 0.6098123788833618, 'eval_f1': 0.0, 'eval_runtime': 7.7575, 'eval_samples_per_second': 235.771, 'eval_steps_per_second': 1.934, 'epoch': 1.0}\n","\n","  2% 58/3480 [01:27<56:36,  1.01it/s]\n","  2% 71/3480 [01:44<1:16:15,  1.34s/it]Traceback (most recent call last):\n","  File \"train.py\", line 88, in <module>\n","    main()\n","  File \"/usr/local/lib/python3.8/dist-packages/hydra/main.py\", line 90, in decorated_main\n","    _run_hydra(\n","  File \"/usr/local/lib/python3.8/dist-packages/hydra/_internal/utils.py\", line 394, in _run_hydra\n","    _run_app(\n","  File \"/usr/local/lib/python3.8/dist-packages/hydra/_internal/utils.py\", line 457, in _run_app\n","    run_and_report(\n","  File \"/usr/local/lib/python3.8/dist-packages/hydra/_internal/utils.py\", line 219, in run_and_report\n","    return func()\n","  File \"/usr/local/lib/python3.8/dist-packages/hydra/_internal/utils.py\", line 458, in <lambda>\n","    lambda: hydra.run(\n","  File \"/usr/local/lib/python3.8/dist-packages/hydra/_internal/hydra.py\", line 119, in run\n","    ret = run_job(\n","  File \"/usr/local/lib/python3.8/dist-packages/hydra/core/utils.py\", line 186, in run_job\n","    ret.return_value = task_function(task_cfg)\n","  File \"train.py\", line 78, in main\n","    trainer.train()\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 1543, in train\n","    return inner_training_loop(\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 1791, in _inner_training_loop\n","    tr_loss_step = self.training_step(model, inputs)\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 2557, in training_step\n","    loss.backward()\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\", line 488, in backward\n","    torch.autograd.backward(\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\", line 197, in backward\n","    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n","KeyboardInterrupt\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 255).\u001b[0m Press Control-C to abort syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               eval/loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            eval/runtime ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: eval/samples_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   eval/steps_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             train/epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train/global_step ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/f1 0.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:               eval/loss 0.60981\n","\u001b[34m\u001b[1mwandb\u001b[0m:            eval/runtime 7.7575\n","\u001b[34m\u001b[1mwandb\u001b[0m: eval/samples_per_second 235.771\n","\u001b[34m\u001b[1mwandb\u001b[0m:   eval/steps_per_second 1.934\n","\u001b[34m\u001b[1mwandb\u001b[0m:             train/epoch 1.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train/global_step 58\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbatch128_epoch60\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/textnet/MemeProject/runs/8j7o8xh3\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230125_094648-8j7o8xh3/logs\u001b[0m\n","^C\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"4jT7p25KoLli"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3VZtNtLchrEr"},"execution_count":null,"outputs":[]}]}