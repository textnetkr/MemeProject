{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyOaVyoJAU0rTVHFsskbeHZ5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"premium","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Install"],"metadata":{"id":"FN-Jm9IgyynN"}},{"cell_type":"code","source":["!pip3 install hydra-core"],"metadata":{"id":"g376Z7kty0g5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip3 install transformers"],"metadata":{"id":"cc96tadMzKCR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip3 install datasets"],"metadata":{"id":"BZ8RWnMHrtVA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip3 install pshmodule"],"metadata":{"id":"yzC54sGWPm3t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip3 install wandb"],"metadata":{"id":"ElwnbM7TWVCJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"qqomZDQhz488","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674628219429,"user_tz":-540,"elapsed":40919,"user":{"displayName":"박성환","userId":"16608640204304641951"}},"outputId":"61147a0e-51d2-4d16-e1f5-aa3bd64c45ff"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# GPU Check"],"metadata":{"id":"Fiap6RttXJza"}},{"cell_type":"code","source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LXqlHhTzXz49","executionInfo":{"status":"ok","timestamp":1674628225974,"user_tz":-540,"elapsed":1380,"user":{"displayName":"박성환","userId":"16608640204304641951"}},"outputId":"b5720379-31f7-45a8-bfd3-22899da463ae"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Jan 25 06:30:25 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   31C    P0    49W / 400W |      0MiB / 40960MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lbRds2i6X5Hl","executionInfo":{"status":"ok","timestamp":1674628225975,"user_tz":-540,"elapsed":3,"user":{"displayName":"박성환","userId":"16608640204304641951"}},"outputId":"0c3bdfa9-53cd-4898-91c1-8a46192de4b3"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Your runtime has 89.6 gigabytes of available RAM\n","\n","You are using a high-RAM runtime!\n"]}]},{"cell_type":"markdown","source":["# Run"],"metadata":{"id":"1D-Ls5bAXRVN"}},{"cell_type":"code","source":["import os\n","os.chdir('/content/drive/MyDrive/MemeProject/src/pytorch/')\n","\n","!python3 train.py"],"metadata":{"id":"ACD6i5ZdXJGa","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1b1e5f92-4bce-4aa9-b1bf-40b136cb3e42"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-01-25 06:30:30.986866: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","train.py:16: UserWarning: \n","The version_base parameter is not specified.\n","Please specify a compatability version level, or None.\n","Will assume defaults for version 1.1\n","  @hydra.main(config_path=\"./\", config_name=\"config\")\n","/usr/local/lib/python3.8/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n","See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n","  ret = run_job(\n","Downloading: 100% 288/288 [00:00<00:00, 404kB/s]\n","Downloading: 100% 504/504 [00:00<00:00, 582kB/s]\n","Downloading: 100% 450k/450k [00:00<00:00, 4.06MB/s]\n","Downloading: 100% 124/124 [00:00<00:00, 151kB/s]\n","Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-86767ae1177a0623/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n","Downloading data files: 100% 1/1 [00:00<00:00, 3002.37it/s]\n","Extracting data files: 100% 1/1 [00:00<00:00, 168.59it/s]\n","Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-86767ae1177a0623/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n","100% 2/2 [00:00<00:00, 343.56it/s]\n","#0:   0% 0/15 [00:00<?, ?ba/s]\n","#1:   0% 0/15 [00:00<?, ?ba/s]\u001b[A\n","\n","#2:   0% 0/15 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","#0:  20% 3/15 [00:00<00:00, 27.71ba/s]\n","#1:  27% 4/15 [00:00<00:00, 34.95ba/s]\u001b[A\n","\n","#2:  33% 5/15 [00:00<00:00, 43.43ba/s]\u001b[A\u001b[A\n","\n","\n","#0:  60% 9/15 [00:00<00:00, 44.96ba/s]\n","#1:  67% 10/15 [00:00<00:00, 47.63ba/s]\u001b[A\n","\n","#2:  73% 11/15 [00:00<00:00, 51.30ba/s]\u001b[A\u001b[A\n","\n","\n","#1: 100% 15/15 [00:00<00:00, 51.01ba/s]\n","#2: 100% 15/15 [00:00<00:00, 54.13ba/s]\n","#0: 100% 15/15 [00:00<00:00, 49.06ba/s]\n","#3: 100% 15/15 [00:00<00:00, 55.95ba/s]\n","#0:   0% 0/4 [00:00<?, ?ba/s]\n","#1:   0% 0/4 [00:00<?, ?ba/s]\u001b[A\n","\n","#2:   0% 0/4 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","#0: 100% 4/4 [00:00<00:00, 48.62ba/s]\n","#1: 100% 4/4 [00:00<00:00, 49.69ba/s]\n","#2: 100% 4/4 [00:00<00:00, 50.53ba/s]\n","#3: 100% 4/4 [00:00<00:00, 49.77ba/s]\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/MemeProject/src/pytorch/outputs/2023-01-25/06-30-37/wandb/run-20230125_063109-bz36cmja\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbatch128_epoch60\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/textnet/MemeProject\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/textnet/MemeProject/runs/bz36cmja\u001b[0m\n","Downloading: 100% 511M/511M [00:05<00:00, 100MB/s]\n","Some weights of the model checkpoint at beomi/KcELECTRA-base-v2022 were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias']\n","- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base-v2022 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 7316\n","  Num Epochs = 60\n","  Instantaneous batch size per device = 128\n","  Total train batch size (w. parallel, distributed & accumulation) = 128\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3480\n","  Number of trainable parameters = 128949493\n","Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  2% 58/3480 [01:18<56:45,  1.00it/s]  ***** Running Evaluation *****\n","  Num examples = 1829\n","  Batch size = 128\n","\n","  0% 0/15 [00:00<?, ?it/s]\u001b[A\n"," 13% 2/15 [00:00<00:03,  3.95it/s]\u001b[A\n"," 20% 3/15 [00:01<00:04,  2.79it/s]\u001b[A\n"," 27% 4/15 [00:01<00:04,  2.42it/s]\u001b[A\n"," 33% 5/15 [00:02<00:04,  2.25it/s]\u001b[A\n"," 40% 6/15 [00:02<00:04,  2.15it/s]\u001b[A\n"," 47% 7/15 [00:03<00:03,  2.09it/s]\u001b[A\n"," 53% 8/15 [00:03<00:03,  2.06it/s]\u001b[A\n"," 60% 9/15 [00:04<00:02,  2.03it/s]\u001b[A\n"," 67% 10/15 [00:04<00:02,  2.02it/s]\u001b[A\n"," 73% 11/15 [00:05<00:01,  2.01it/s]\u001b[A\n"," 80% 12/15 [00:05<00:01,  2.00it/s]\u001b[A\n"," 87% 13/15 [00:06<00:01,  2.00it/s]\u001b[A\n"," 93% 14/15 [00:06<00:00,  1.99it/s]\u001b[A\n","100% 15/15 [00:06<00:00,  2.52it/s]\u001b[Atrain.py:43: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","  metric = load_metric(cfg.METRICS.metric_name)\n","\n","\n","Downloading builder script: 6.50kB [00:00, 5.00MB/s]       \n","Error executing job with overrides: []\n","Traceback (most recent call last):\n","  File \"train.py\", line 62, in main\n","    trainer.train()\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 1543, in train\n","    return inner_training_loop(\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 1883, in _inner_training_loop\n","    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 2131, in _maybe_log_save_evaluate\n","    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 2827, in evaluate\n","    output = eval_loop(\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 3116, in evaluation_loop\n","    metrics = self.compute_metrics(EvalPrediction(predictions=all_preds, label_ids=all_labels))\n","  File \"train.py\", line 47, in compute_metrics\n","    return metric.compute(\n","  File \"/usr/local/lib/python3.8/dist-packages/datasets/metric.py\", line 442, in compute\n","    self.add_batch(**inputs)\n","  File \"/usr/local/lib/python3.8/dist-packages/datasets/metric.py\", line 494, in add_batch\n","    batch = self.info.features.encode_batch(batch)\n","  File \"/usr/local/lib/python3.8/dist-packages/datasets/features/features.py\", line 1818, in encode_batch\n","    encoded_batch[key] = [encode_nested_example(self[key], obj) for obj in column]\n","  File \"/usr/local/lib/python3.8/dist-packages/datasets/features/features.py\", line 1818, in <listcomp>\n","    encoded_batch[key] = [encode_nested_example(self[key], obj) for obj in column]\n","  File \"/usr/local/lib/python3.8/dist-packages/datasets/features/features.py\", line 1257, in encode_nested_example\n","    return schema.encode_example(obj) if obj is not None else None\n","  File \"/usr/local/lib/python3.8/dist-packages/datasets/features/features.py\", line 498, in encode_example\n","    return int(value)\n","TypeError: only size-1 arrays can be converted to Python scalars\n","\n","Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n","Error in atexit._run_exitfuncs:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.8/subprocess.py\", line 1083, in wait\n","    return self._wait(timeout=timeout)\n","  File \"/usr/lib/python3.8/subprocess.py\", line 1806, in _wait\n","    (pid, sts) = self._try_wait(0)\n","  File \"/usr/lib/python3.8/subprocess.py\", line 1764, in _try_wait\n","    (pid, sts) = os.waitpid(self.pid, wait_flags)\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/wandb_manager.py\", line 152, in _teardown\n","    result = self._service.join()\n","  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/service/service.py\", line 154, in join\n","    ret = self._internal_proc.wait()\n","  File \"/usr/lib/python3.8/subprocess.py\", line 1096, in wait\n","    self._wait(timeout=sigint_timeout)\n","  File \"/usr/lib/python3.8/subprocess.py\", line 1800, in _wait\n","    time.sleep(delay)\n","KeyboardInterrupt\n","  2% 58/3480 [01:33<1:31:50,  1.61s/it]\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"4jT7p25KoLli"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3VZtNtLchrEr"},"execution_count":null,"outputs":[]}]}